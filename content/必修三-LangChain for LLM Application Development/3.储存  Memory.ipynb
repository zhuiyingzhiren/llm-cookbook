{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a786c77c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 第三章 储存\n",
    "\n",
    " - [一、设置OpenAI API Key](#一、设置OpenAI-API-Key)\n",
    " - [二、对话缓存储存](#二、对话缓存储存)\n",
    "     - [2.1 英文版](#2.1-英文版)\n",
    "         - [2.1.1 初始化对话模型](#2.1.1-初始化对话模型)\n",
    "         - [2.1.2 第一轮对话](#2.1.2-第一轮对话)\n",
    "         - [2.1.3 第二轮对话](#2.1.3-第二轮对话)\n",
    "         - [2.1.4 第三轮对话](#2.1.4-第三轮对话)\n",
    "         - [2.1.5 查看储存缓存](#2.1.5-查看储存缓存)\n",
    "         - [2.1.6 直接添加内容到储存缓存](#2.1.6-直接添加内容到储存缓存)\n",
    "     - [2.2 中文版](#2.2-中文版)\n",
    "         - [2.1.1 初始化对话模型](#2.1.1-初始化对话模型)\n",
    "         - [2.1.2 第一轮对话](#2.1.2-第一轮对话)\n",
    "         - [2.1.3 第二轮对话](#2.1.3-第二轮对话)\n",
    "         - [2.1.4 第三轮对话](#2.1.4-第三轮对话)\n",
    "         - [2.1.5 查看储存缓存](#2.1.5-查看储存缓存)\n",
    "         - [2.1.6 直接添加内容到储存缓存](#2.1.6-直接添加内容到储存缓存)\n",
    "     - [2.3 总结](#2.3-总结)\n",
    " - [三、对话缓存窗口储存](#三、对话缓存窗口储存)\n",
    "     - [3.1 英文版](#3.1-英文版)\n",
    "         - [3.1.1 添加两轮对话到窗口储存](#3.1.1-添加两轮对话到窗口储存)\n",
    "         - [3.1.2 在对话链中应用窗口储存](#3.1.2-在对话链中应用窗口储存)\n",
    "     - [3.2 中文版](#3.2-中文版)\n",
    "         - [3.1.1 添加两轮对话到窗口储存](#3.1.1-添加两轮对话到窗口储存)\n",
    "         - [3.1.2 在对话链中应用窗口储存](#3.1.2-在对话链中应用窗口储存)\n",
    " - [四、对话token缓存储存](#四、对话token缓存储存)\n",
    "     - [4.1 英文版](#4.1-英文版)\n",
    "     - [4.2 中文版](#4.2-中文版)\n",
    "     - [4.3 补充](#4.3-补充)\n",
    " - [五、对话摘要缓存储存](#五、对话摘要缓存储存)\n",
    "     - [5.1 英文版](#5.1-英文版)\n",
    "         - [5.1.1 使用对话摘要缓存储存](#5.1.1-使用对话摘要缓存储存)\n",
    "         - [5.1.2 基于对话摘要缓存储存的对话链](#5.1.2-基于对话摘要缓存储存的对话链)\n",
    "     - [5.2 中文版](#5.2-中文版)\n",
    "         - [5.2.1 使用对话摘要缓存储存](#5.2.1-使用对话摘要缓存储存)\n",
    "         - [5.1.2 基于对话摘要缓存储存的对话链](#5.1.2-基于对话摘要缓存储存的对话链)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e10db6f",
   "metadata": {},
   "source": [
    "当你与那些语言模型进行交互的时候，他们不会记得你之前和他进行的交流内容，这在我们构建一些应用程序（如聊天机器人）的时候，是一个很大的问题 -- 显得不够智能！因此，在本节中我们将介绍 LangChain 中的储存模块，即如何将先前的对话嵌入到语言模型中的，使其具有连续对话的能力。\n",
    "\n",
    "当使用 LangChain 中的<span style=\"color:red\">储存(Memory)模块</span>时，它可以帮助保存和管理历史聊天消息，以及构建关于特定实体的知识。这些组件可以跨多轮对话储存信息，并允许在对话期间跟踪特定信息和上下文。\n",
    "\n",
    "LangChain 提供了多种储存类型。其中，缓冲区储存允许保留最近的聊天消息，摘要储存则提供了对整个对话的摘要。实体储存 则允许在多轮对话中保留有关特定实体的信息。这些记忆组件都是模块化的，可与其他组件组合使用，从而增强机器人的对话管理能力。储存模块可以通过简单的API调用来访问和更新，允许开发人员更轻松地实现对话历史记录的管理和维护。\n",
    "\n",
    "此次课程主要介绍其中四种储存模块，其他模块可查看文档学习。\n",
    "- 对话缓存储存 (ConversationBufferMemory）\n",
    "- 对话缓存窗口储存 (ConversationBufferWindowMemory）\n",
    "- 对话令牌缓存储存 (ConversationTokenBufferMemory）\n",
    "- 对话摘要缓存储存 (ConversationSummaryBufferMemory）\n",
    "\n",
    "在LangChain中，储存 指的是大语言模型（LLM）的<span style=\"color:red\">短期记忆</span>。为什么是短期记忆？那是因为LLM训练好之后 (获得了一些长期记忆)，它的参数便不会因为用户的输入而发生改变。当用户与训练好的LLM进行对话时，LLM会暂时记住用户的输入和它已经生成的输出，以便预测之后的输出，而模型输出完毕后，它便会“遗忘”之前用户的输入和它的输出。因此，之前的这些信息只能称作为LLM的短期记忆。  \n",
    "  \n",
    "为了延长LLM短期记忆的保留时间，则需要借助一些外部储存方式来进行记忆，以便在用户与LLM对话中，LLM能够尽可能的知道用户与它所进行的历史对话信息。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca56e6b-1e07-4405-a1ca-f4237f20fa75",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 一、设置OpenAI API Key\n",
    "\n",
    "登陆 [OpenAI 账户](https://platform.openai.com/account/api-keys) 获取API Key，然后将其设置为环境变量。\n",
    "\n",
    "- 如果你想要设置为全局环境变量，可以参考[知乎文章](https://zhuanlan.zhihu.com/p/627665725)。\n",
    "- 如果你想要设置为本地/项目环境变量，在本文件目录下创建`.env`文件, 打开文件输入以下内容。\n",
    "\n",
    "    <p style=\"font-family:verdana; font-size:12px;color:green\">\n",
    "    OPENAI_API_KEY=\"your_api_key\" \n",
    "    </p>\n",
    "  \n",
    "  替换\"your_api_key\"为你自己的 API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10446712-9fa6-4d71-94ce-2ea4cf197e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# 读取本地/项目的环境变量。\n",
    "\n",
    "# find_dotenv()寻找并定位.env文件的路径\n",
    "# load_dotenv()读取该.env文件，并将其中的环境变量加载到当前的运行环境中  \n",
    "# 如果你设置的是全局的环境变量，这行代码则没有任何作用。\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# 获取环境变量 OPENAI_API_KEY\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297dcd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 二、对话缓存储存\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e77a3d-7aaa-48c1-b219-19bd6f4eb674",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 初始化对话模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20ad6fe2",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.base import ConversationChain\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.memory.buffer import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5d3bec",
   "metadata": {},
   "source": [
    "<span style=\"color:#00ff00\">\n",
    "在LangChain中，不同的链和模型类可能使用不同的参数名称。对于ConversationChain，正确的参数名称是llm而不是model。\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88bdf13d",
   "metadata": {
    "height": 133
   },
   "outputs": [],
   "source": [
    "# 这里我们将参数temperature设置为0.0，从而减少生成答案的随机性。\n",
    "# 如果你想要每次得到不一样的有新意的答案，可以尝试增大该参数。\n",
    "llm = init_chat_model(\"gpt-4o-mini\",model_provider=\"openai\", temperature=0.7)\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# 新建一个 ConversationChain Class 实例\n",
    "# verbose参数设置为True时，程序会输出更详细的信息，以提供更多的调试或运行时信息。\n",
    "# 相反，当将verbose参数设置为False时，程序会以更简洁的方式运行，只输出关键的信息。\n",
    "conversation = ConversationChain(llm=llm, memory = memory, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea83837",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 第一轮对话"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3b4c42",
   "metadata": {},
   "source": [
    "当我们运行<span style=\"color:red\">预测(predict)</span>时，生成了一些提示，如下所见，他说“以下是人类和AI之间友好的对话，AI健谈“等等，这实际上是LangChain生成的提示，以使系统进行希望和友好的对话，并且必须保存对话，并提示了当前已完成的模型链。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db24677d",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 你好, 我叫皮皮鲁\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你好，皮皮鲁！很高兴认识你！我是一个人工智能助手，随时准备帮助你。你今天过得怎么样？有没有什么特别想聊的？'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"你好, 我叫皮皮鲁\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71564ad",
   "metadata": {},
   "source": [
    "### 2.3 第二轮对话"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d006bd",
   "metadata": {},
   "source": [
    "当我们进行第二轮对话时，它会<span style=\"color:red\">保留上面的提示</span>。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc3ef937",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好, 我叫皮皮鲁\n",
      "AI: 你好，皮皮鲁！很高兴认识你！我是一种人工智能助手，随时准备回答你的问题或聊聊天。你今天过得怎么样？有没有什么特别的事情想分享的？\n",
      "Human: 1+1等于多少？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1等于2！这是一个简单的数学问题。如果你对数学有更多的问题或者想讨论其他主题，随时告诉我！'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"1+1等于多少？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb734b",
   "metadata": {},
   "source": [
    "### 2.4 第三轮对话"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393df3d",
   "metadata": {},
   "source": [
    "为了验证他是否记忆了前面的对话内容，我们让他回答前面已经说过的内容（我的名字），可以看到他确实输出了正确的名字，因此这个对话链随着往下进行会越来越长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acf3339a",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好, 我叫皮皮鲁\n",
      "AI: 你好，皮皮鲁！很高兴认识你！我是一种人工智能助手，随时准备回答你的问题或聊聊天。你今天过得怎么样？有没有什么特别的事情想分享的？\n",
      "Human: 1+1等于多少？\n",
      "AI: 1+1等于2！这是一个简单的数学问题。如果你对数学有更多的问题或者想讨论其他主题，随时告诉我！\n",
      "Human: 我叫什么名字？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你叫皮皮鲁！我记得你刚刚告诉我你的名字。如果你还有其他问题或者想聊聊其他事情，尽管问我！'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"我叫什么名字？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a96a8d9",
   "metadata": {},
   "source": [
    "### 2.5 查看储存缓存\n",
    "\n",
    "<span style=\"color:red\">储存缓存（memory.buffer）</span>\n",
    "储存了当前为止所有的对话信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2529400d",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 你好, 我叫皮皮鲁\n",
      "AI: 你好，皮皮鲁！很高兴认识你！我是一种人工智能助手，随时准备回答你的问题或聊聊天。你今天过得怎么样？有没有什么特别的事情想分享的？\n",
      "Human: 1+1等于多少？\n",
      "AI: 1+1等于2！这是一个简单的数学问题。如果你对数学有更多的问题或者想讨论其他主题，随时告诉我！\n",
      "Human: 我叫什么名字？\n",
      "AI: 你叫皮皮鲁！我记得你刚刚告诉我你的名字。如果你还有其他问题或者想聊聊其他事情，尽管问我！\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5de846",
   "metadata": {},
   "source": [
    "也可以通过<span style=\"color:red\">memory.load_memory_variables({})</span>打印缓存中的历史消息。这里的`{}`是一个空字典，有一些更高级的功能，使用户可以使用更复杂的输入，但我们不会在这个短期课程中讨论它们，所以不要担心为什么这里有一个空的花括号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5018cb0a",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: 你好, 我叫皮皮鲁\\nAI: 你好，皮皮鲁！很高兴认识你！我是一种人工智能助手，随时准备回答你的问题或聊聊天。你今天过得怎么样？有没有什么特别的事情想分享的？\\nHuman: 1+1等于多少？\\nAI: 1+1等于2！这是一个简单的数学问题。如果你对数学有更多的问题或者想讨论其他主题，随时告诉我！\\nHuman: 我叫什么名字？\\nAI: 你叫皮皮鲁！我记得你刚刚告诉我你的名字。如果你还有其他问题或者想聊聊其他事情，尽管问我！'}\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d2e892",
   "metadata": {},
   "source": [
    "### 2.6 直接添加内容到储存缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14219b70",
   "metadata": {
    "height": 31
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()  # 新建一个空的对话缓存记忆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a36e9905",
   "metadata": {
    "height": 48
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"你好，我叫皮皮鲁\"}, {\"output\": \"你好啊，我叫鲁西西\"}) # 向缓存区添加指定对话的输入输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61631b1f",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: 你好，我叫皮皮鲁\\nAI: 你好啊，我叫鲁西西'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})   # 查看缓存区结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2fdf9ec",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: 你好，我叫皮皮鲁\\nAI: 你好啊，我叫鲁西西'}\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({}))# 再次加载记忆变量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac544f2",
   "metadata": {},
   "source": [
    "继续添加新的内容，对话历史都保存下来在了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ca79256",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"很高兴和你成为朋友！\"}, {\"output\": \"是的，让我们一起去冒险吧！\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "890a4497",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: 你好，我叫皮皮鲁\\nAI: 你好啊，我叫鲁西西\\nHuman: 很高兴和你成为朋友！\\nAI: 是的，让我们一起去冒险吧！'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7eba04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: 你好，我叫皮皮鲁\\nAI: 你好啊，我叫鲁西西\\nHuman: 很高兴和你成为朋友！\\nAI: 是的，让我们一起去冒险吧！'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10146f1a-0114-4902-8122-d19ae6f7c461",
   "metadata": {},
   "source": [
    "### 2.7 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2759b6bc-edb2-4cfe-b0f8-1bf6c4d796f9",
   "metadata": {},
   "source": [
    "当我们在使用大型语言模型进行聊天对话时，**大型语言模型本身实际上是无状态的。语言模型本身并不记得到目前为止的历史对话**。每次调用API结点都是独立的。储存(Memory)可以储存到目前为止的所有术语或对话，并将其<span style=\"color:red\">输入或附加上下文</span>到LLM中用于生成输出。如此看起来就好像它在进行下一轮对话的时候，记得之前说过什么。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98e9ff",
   "metadata": {},
   "source": [
    "## 三、对话缓存窗口储存\n",
    "  \n",
    "随着对话变得越来越长，所需的内存量也变得非常长。将大量的tokens发送到LLM的成本，也会变得更加昂贵,这也就是为什么API的调用费用，通常是基于它需要处理的tokens数量而收费的。\n",
    "  \n",
    "针对以上问题，LangChain也提供了几种方便的储存方式来保存历史对话。其中，对话缓存窗口储存只保留一个窗口大小的对话。它<span style=\"color:red\">只使用最近的n次交互</span>。这可以用于保持最近交互的滑动窗口，以便缓冲区不会过大"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641477a4",
   "metadata": {},
   "source": [
    "### 3.1 添加两轮对话到窗口储存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ea6233e",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# k 为窗口参数，k=1表明只保留一个对话记忆\n",
    "memory = ConversationBufferWindowMemory(k=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc4553fb",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "# 向memory添加两轮对话\n",
    "memory.save_context({\"input\": \"你好，我叫皮皮鲁\"}, {\"output\": \"你好啊，我叫鲁西西\"})\n",
    "memory.save_context({\"input\": \"很高兴和你成为朋友！\"}, {\"output\": \"是的，让我们一起去冒险吧！\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#00ff00\">\n",
    "由于k=1，因此只能保存一轮的历史消息。\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a788403",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: 很高兴和你成为朋友！\\nAI: 是的，让我们一起去冒险吧！'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 并查看记忆变量当前的记录\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bda148",
   "metadata": {},
   "source": [
    "### 3.2 在对话链中应用窗口储存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4087bc87",
   "metadata": {
    "height": 133
   },
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"gpt-4o-mini\",model_provider=\"openai\", temperature=0.7)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d661e3",
   "metadata": {},
   "source": [
    "注意此处！由于这里用的是一个窗口的记忆，因此只能保存一轮的历史消息，因此AI并不能知道你第一轮对话中提到的名字，他最多只能记住上一轮（第二轮）的对话信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4faaa952",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 你好, 我叫皮皮鲁\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "你好，皮皮鲁！很高兴认识你！我叫AI助手，随时准备和你聊天。你今天过得怎么样？有什么特别的事情想分享吗？\n"
     ]
    }
   ],
   "source": [
    "print(conversation.predict(input=\"你好, 我叫皮皮鲁\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb20ddaa",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好, 我叫皮皮鲁\n",
      "AI: 你好，皮皮鲁！很高兴认识你！我叫AI助手，随时准备和你聊天。你今天过得怎么样？有什么特别的事情想分享吗？\n",
      "Human: 1+1等于多少？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1+1等于2！这是一个简单的数学问题。如果你对数学或者其他学科有更多的问题，随时问我哦！\n"
     ]
    }
   ],
   "source": [
    "print(conversation.predict(input=\"1+1等于多少？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d25381",
   "metadata": {},
   "source": [
    "<span style=\"color:#00ff00\">\n",
    "可以看到，历史记录中的第一轮对话信息不见了，AI不再知道第一轮对话中提到的名字。\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "489b2194",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 1+1等于多少？\n",
      "AI: 1+1等于2！这是一个简单的数学问题。如果你对数学或者其他学科有更多的问题，随时问我哦！\n",
      "Human: 我叫什么名字？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "抱歉，我不知道你的名字。你可以告诉我你的名字吗？或者如果你有其他问题，我也很乐意回答！\n"
     ]
    }
   ],
   "source": [
    "print(conversation.predict(input=\"我叫什么名字？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2931b92",
   "metadata": {},
   "source": [
    "## 四、对话token缓存储存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff5b4c7",
   "metadata": {},
   "source": [
    "使用对话token缓存记忆，内存将<span style=\"color:red\">限制保存的token数量</span>。如果token数量超出指定数目，它会切掉这个对话的早期部分\n",
    "以保留与最近的交流相对应的token数量，但不超过token限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb9020ed",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "from langchain.memory.token_buffer import ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a84112",
   "metadata": {},
   "source": [
    "### 4.1 演示\n",
    "添加对话到Token缓存储存,限制token数量，进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "43582ee6",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)\n",
    "memory.save_context({\"input\": \"朝辞白帝彩云间，\"}, {\"output\": \"千里江陵一日还。\"})\n",
    "memory.save_context({\"input\": \"两岸猿声啼不住，\"}, {\"output\": \"轻舟已过万重山。\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "284288e1",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: 两岸猿声啼不住，\\nAI: 轻舟已过万重山。'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b62b2e1",
   "metadata": {},
   "source": [
    "可以看到前面超出的的token已经被舍弃了！！！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb08ef4a-876f-422a-81f9-4805288e5955",
   "metadata": {},
   "source": [
    "### 4.2 补充"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d918b",
   "metadata": {},
   "source": [
    "ChatGPT使用一种基于<span style=\"color:red\">字节对编码（Byte Pair Encoding，BPE）</span>的方法来进行tokenization（将输入文本拆分为token）。BPE是一种常见的tokenization技术，它将输入文本分割成较小的子词单元。 \n",
    "\n",
    "OpenAI在其官方GitHub上公开了一个最新的开源Python库 [tiktoken](https://github.com/openai/tiktoken)，这个库主要是用来计算tokens数量的。相比较HuggingFace的tokenizer，其速度提升了好几倍。\n",
    "\n",
    "具体token计算方式,特别是汉字和英文单词的token区别，具体课参考[知乎文章](https://www.zhihu.com/question/594159910) 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b97568",
   "metadata": {},
   "source": [
    "<span style=\"color:#00ff00\">\n",
    "使用Claude 3.7 Sonnet模型生成一段代码计算token数量和token划分信息。可以看到，ConversationTokenBufferMemory只有当当前的完整一次回复（AI或human）不会超过token数量限制时，才会保留该句话的对话信息。\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b4b2420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini模型的token数量: 23\n",
      "\n",
      "每个token的详细信息:\n",
      "Token 1: ID=48426, 文本='Human'\n",
      "Token 2: ID=25, 文本=':'\n",
      "Token 3: ID=117762, 文本=' 两'\n",
      "Token 4: ID=86845, 文本='岸'\n",
      "Token 5: ID=15857, 文本='�'\n",
      "Token 6: ID=123, 文本='�'\n",
      "Token 7: ID=26968, 文本='声'\n",
      "Token 8: ID=5510, 文本='�'\n",
      "Token 9: ID=120, 文本='�'\n",
      "Token 10: ID=3428, 文本='不'\n",
      "Token 11: ID=25503, 文本='住'\n",
      "Token 12: ID=51663, 文本='，\n",
      "'\n",
      "Token 13: ID=17527, 文本='AI'\n",
      "Token 14: ID=25, 文本=':'\n",
      "Token 15: ID=220, 文本=' '\n",
      "Token 16: ID=49334, 文本='轻'\n",
      "Token 17: ID=182002, 文本='舟'\n",
      "Token 18: ID=18601, 文本='已'\n",
      "Token 19: ID=10779, 文本='过'\n",
      "Token 20: ID=6848, 文本='万'\n",
      "Token 21: ID=4948, 文本='重'\n",
      "Token 22: ID=11767, 文本='山'\n",
      "Token 23: ID=788, 文本='。'\n"
     ]
    }
   ],
   "source": [
    "content =  \"\"\"Human: 两岸猿声啼不住，\\nAI: 轻舟已过万重山。\"\"\"\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# 使用gpt-4o-mini的编码器\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "# 对内容进行tokenization\n",
    "tokens = encoding.encode(content)\n",
    "\n",
    "# 打印token数量\n",
    "print(f\"gpt-4o-mini模型的token数量: {len(tokens)}\")\n",
    "\n",
    "# 打印每个token的编码和对应的文本\n",
    "print(\"\\n每个token的详细信息:\")\n",
    "for i, token in enumerate(tokens):\n",
    "    # 解码单个token以查看其对应的文本\n",
    "    token_text = encoding.decode([token])\n",
    "    print(f\"Token {i+1}: ID={token}, 文本='{token_text}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff55d5d",
   "metadata": {},
   "source": [
    "## 五、对话摘要缓存储存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d39b83a",
   "metadata": {},
   "source": [
    "对话摘要缓存储存，**<span style=\"color:red\">使用LLM编写</span>到目前为止历史对话的摘要**，并将其保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "72dcf8b1",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from langchain.memory.summary_buffer import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572ef39",
   "metadata": {},
   "source": [
    "### 5.1 使用对话摘要缓存储存\n",
    "\n",
    "创建一个长字符串，其中包含某人的日程安排"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4a5b238f",
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "# 创建一个长字符串\n",
    "schedule = \"在八点你和你的产品团队有一个会议。 \\\n",
    "你需要做一个PPT。 \\\n",
    "上午9点到12点你需要忙于LangChain。\\\n",
    "Langchain是一个有用的工具，因此你的项目进展的非常快。\\\n",
    "中午，在意大利餐厅与一位开车来的顾客共进午餐 \\\n",
    "走了一个多小时的路程与你见面，只为了解最新的 AI。 \\\n",
    "确保你带了笔记本电脑可以展示最新的 LLM 样例.\"\n",
    "\n",
    "# 使用对话摘要缓存记忆\n",
    "llm = init_chat_model(\"gpt-4o-mini\",model_provider=\"openai\", temperature=0.7)\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100) \n",
    "memory.save_context({\"input\": \"你好\"}, {\"output\": \"怎么了\"})\n",
    "memory.save_context({\"input\": \"没什么，只是闲着\"}, {\"output\": \"很棒\"})\n",
    "memory.save_context(\n",
    "    {\"input\": \"今天的日程安排是什么？\"}, {\"output\": f\"{schedule}\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4515f20",
   "metadata": {},
   "source": [
    "<span style=\"color:#00ff00\">\n",
    "默认的总结用英文总结\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15226a41-ab36-43a0-93f7-c03c6b374936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The human greets the AI, and they exchange casual remarks about being idle. The human inquires about their schedule for the day. The AI outlines the plan, mentioning a meeting with the product team at 8 AM, the need to prepare a PPT, work on LangChain from 9 AM to 12 PM, a lunch meeting at an Italian restaurant with a customer interested in the latest AI, and a reminder to bring a laptop to showcase the latest LLM examples.\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af66ab58",
   "metadata": {},
   "source": [
    "<span style=\"color:#00ff00\">\n",
    "利用Claude 3.7 Sonnet模型生成一段代码，使用中文进行摘要总结。\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d324ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chinese_summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"summary\", \"new_lines\"],\n",
    "    template=\"\"\"请用中文总结以下对话，并将新加入的内容整合到摘要中：\n",
    "\n",
    "当前对话摘要:\n",
    "{summary}\n",
    "\n",
    "新对话内容:\n",
    "{new_lines}\n",
    "\n",
    "新的中文摘要:\"\"\"\n",
    ")\n",
    "\n",
    "# 初始化模型和记忆组件\n",
    "llm = init_chat_model(\"gpt-4o-mini\",model_provider=\"openai\", temperature=0.7)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm, \n",
    "    max_token_limit=100,\n",
    "    prompt=chinese_summary_prompt  # 使用中文摘要提示模板\n",
    ")\n",
    "memory.save_context({\"input\": \"你好\"}, {\"output\": \"怎么了\"})\n",
    "memory.save_context({\"input\": \"没什么，只是闲着\"}, {\"output\": \"很棒\"})\n",
    "memory.save_context(\n",
    "    {\"input\": \"今天的日程安排是什么？\"}, {\"output\": f\"{schedule}\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05778f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: 在这段对话中，人类与人工智能进行了一次简单的交流。人类首先问候并表示闲着，人工智能回应表示很棒。随后，人类询问今天的日程安排，人工智能提供了详细的安排：早上八点与产品团队有会议，需要准备PPT；九点到十二点忙于LangChain项目；中午与一位顾客在意大利餐厅共进午餐，顾客专程前来了解最新的AI，并提醒人类带上笔记本电脑展示最新的LLM样例。\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb97b6",
   "metadata": {},
   "source": [
    "### 5.2 基于对话摘要缓存储存的对话链\n",
    "基于上面的对话摘要缓存储存，新建一个对话链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6728edba",
   "metadata": {
    "height": 99
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bb582617",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: 在这段对话中，人类与人工智能进行了一次简单的交流。人类首先问候并表示闲着，人工智能回应表示很棒。随后，人类询问今天的日程安排，人工智能提供了详细的安排：早上八点与产品团队有会议，需要准备PPT；九点到十二点忙于LangChain项目；中午与一位顾客在意大利餐厅共进午餐，顾客专程前来了解最新的AI，并提醒人类带上笔记本电脑展示最新的LLM样例。\n",
      "Human: 展示什么样的样例最好呢？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'展示最新的语言模型（LLM）样例时，可以考虑以下几种类型的示例：\\n\\n1. **对话生成**：展示模型如何进行自然流畅的对话。可以准备一些日常场景的对话示例，比如客户服务、技术支持或闲聊。\\n\\n2. **文本生成**：展示模型生成的创意文本，比如故事、诗歌或文章的开头。这可以帮助顾客理解模型的创意潜力。\\n\\n3. **问答系统**：展示模型如何回答特定问题，尤其是行业相关的问题。这可以突出模型的实用性和信息检索能力。\\n\\n4. **代码生成**：如果顾客对技术感兴趣，可以展示模型如何生成代码片段，解决编程问题或实现特定功能。\\n\\n5. **多语言支持**：如果顾客的市场涉及多种语言，可以准备一些不同语言的示例，展示模型的多语言能力。\\n\\n确保在展示时突出模型的优点和应用场景，同时准备好回答顾客可能提出的问题。这样可以帮助他们更好地理解AI的潜力和实际应用。'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"展示什么样的样例最好呢？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0fe0e8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: 在这段对话中，人类与人工智能进行了简单的交流。人类首先问候并表示闲着，人工智能回应表示很棒。人类询问今天的日程安排，人工智能提供了详细的安排：早上八点与产品团队有会议，需要准备PPT；九点到十二点忙于LangChain项目；中午与一位顾客在意大利餐厅共进午餐，顾客专程前来了解最新的AI，并提醒人类带上笔记本电脑展示最新的LLM样例。人类进一步询问展示什么样的样例最好，人工智能建议了几种类型的示例，包括对话生成、文本生成、问答系统、代码生成和多语言支持，并强调展示时要突出模型的优点和应用场景，准备好回答顾客可能提出的问题，以帮助他们更好地理解AI的潜力和实际应用。'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})  # 摘要记录更新了"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
